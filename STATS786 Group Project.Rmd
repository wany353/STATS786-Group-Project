---
title: "STATS 786 Group Project"
author: "Yucheng Wang, Eric Shi, Tongxin Li, Jinze, Li"
date: "2024-05-14"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
library(fpp3)
library(tsibbledata)
library(kableExtra)
library(gt)
library(urca)
Sys.setlocale("LC_TIME", "English_United States.1252")

surrogate.test = function(data, lag, N = 1000, test.stat = "ljung-box") {
  
  # data: a tsibble or numeric vector object
  # lag: number of lags in portmanteau test statistic
  # N: number of permutations to perform
  # test.stat: either "ljung-box" or "box-pierce"
  
  if (is_tsibble(data)) {
    if (length(measures(data)) != 1) {  
      stop("data must be a tsibble with one measurement variable")
    }
    # Extract time series 
    data = data %>% 
      pull(as.character(measures(data)[[1]])) 
  }

  n = length(data)

  Q.null = rep(NA, N)  # Open test statistic vectors
  
  if (test.stat == "ljung-box") {
    
    # Observed test statistic
    r.obs = acf(data, plot = FALSE)$acf[2:(lag + 1)]
    Q.obs = n * (n + 2) * sum(r.obs ^ 2 / (n - 1:lag))
    
    # Null distribution
    for (i in 1:N) {
      surrogate = sample(data, n)  # Permute data (kill autocorrelation, maintain amplitude)
      r = acf(surrogate, plot = FALSE)$acf[2:(lag + 1)]   # Estimate autocorrelation
      Q.null[i] = n * (n + 2) * sum(r ^ 2 / (n - 1:lag))  # Ljung-Box test statistic
    }
    
  }
  
  if (test.stat == "box-pierce") {
    
    # Observed test statistic
    r.obs = acf(data, plot = FALSE)$acf[2:(lag + 1)]
    Q.obs = n * sum(r.obs ^ 2)
    
    # Null distribution
    for (i in 1:N) {
      surrogate = sample(data, n)  # Permute data (kill autocorrelation, maintain amplitude)
      r = acf(surrogate, plot = FALSE)$acf[2:(lag + 1)]  # Estimate autocorrelation
      Q.null[i] = n * sum(r ^ 2)                         # Box-Pierce test statistic
    }
    
  }
  
  # Compute p-value
  p.value = mean(Q.null >= Q.obs)  # p-value

  # Output
  output = list(Q.null = Q.null,
                Q.obs = Q.obs,
                test.stat = test.stat,
                p.value = p.value)
  
  class(output) = "surrogate"
  
  return(output)
  
}

plot.surrogate = function(obj) {
  
  # obj: Object of class "surrogate"

  ggplot(data = data.frame(Q = obj$Q.null),
         mapping = aes(x = Q)) +
    geom_histogram(fill = "navy", colour = "black") +
    geom_vline(xintercept = obj$Q.obs,
               linetype = "dashed") +
    labs(x = "Test statistic",
         y = "Count") 
  
}
```

# 1. Exploratory data analysis
```{r, warning=FALSE}
# read in the data
train <- read_csv('qgdp_training.csv') %>%
  mutate(Date = yearquarter(Date)) %>%
  as_tsibble(index = Date) %>% 
  select(1, 17)


train %>% 
  autoplot()
# STL component plot
train %>%
  model(stl = STL(`Wholesale Trade`, robust = TRUE)) %>%
  components() %>%
  autoplot()
```

# 2. ETS models
```{r}

```


# 3. ARIMA models
## Transformations

```{r}
guerrero(train$`Wholesale Trade`)

train %>%
  autoplot(box_cox(`Wholesale Trade`, -0.14))
```

It looks like multiplicative seasonality is present, so a Box-Cox transformation
was applied. Guerrero's method suggested a lambda parameter of -0.14.

## Differencing

```{r}
train %>%
	features(box_cox(`Wholesale Trade`, -0.14), unitroot_kpss)

train %>%
	features(difference(box_cox(`Wholesale Trade`, -0.14), lag=4), unitroot_kpss)
```

Performing the KPSS test on the Box-Cox wholesale trade variable before
differencing gave a p-value of 0.01, so we have strong evidence against
the null hypothesis that the time series is stationary.

A seasonal difference was applied because it looks like seasonality is present.
Performing the KPSS test on the seasonally differenced series gave a p-value
of 0.1, so we can assume the series is stationary and no further differencing
is required.

```{r}
train %>%
  gg_tsdisplay(difference(box_cox(`Wholesale Trade`, -0.14), lag=4), plot_type="partial")

fit = train %>%
  model(auto = ARIMA(box_cox(`Wholesale Trade`, -0.14), stepwise=F, approximation=F),
        arima100210 = ARIMA(box_cox(`Wholesale Trade`, -0.14) ~ pdq(1,0,0) + PDQ(2,1,0)),
        arima100011 = ARIMA(box_cox(`Wholesale Trade`, -0.14) ~ pdq(1,0,0) + PDQ(0,1,1)),
        arima002210 = ARIMA(box_cox(`Wholesale Trade`, -0.14) ~ pdq(0,0,2) + PDQ(2,1,0)),
        arima002011 = ARIMA(box_cox(`Wholesale Trade`, -0.14) ~ pdq(0,0,2) + PDQ(0,1,1)))

glance(fit) %>% arrange(AICc) %>% select(.model:BIC)

fit$auto
```

To come up with candidate models, we used the ARIMA function to find a model
automatically, as well as manually finding models by considering the ACF and
PACF plots.

The PACF plot has spikes at lags 4 and 8, suggesting 2 seasonal AR terms, and
a spike at lag 1, suggesting 1 non-seasonal AR term. The ACF plot has spikes at
lag 4, suggesting 1 seasonal MA term, and spikes at lags 1 and 2, suggesting 2
non-seasonal MA terms. Therefore, we considered models ARIMA(1,0,0)(0,1,1)[4],
ARIMA(1,0,0)(2,1,0)[4], ARIMA(0,0,2)(0,1,1)[4], and ARIMA(0,0,2)(2,1,0)[4].

Overall, the ARIMA model with the best predictive ability is the automatically
found ARIMA(0,0,3)(0,1,1)[4] due to it having the lowest AICc score.

## Fitted model equation

The model equation in backshift notation is 
$$(1-B^4)w_t = (1+\Theta_1B^4)(1+\theta_1B+\theta_2B^2+\theta_3B^3)
\varepsilon_t$$

where $\varepsilon_t \sim N(0, \sigma^2)$ and
$w_t = (sign(y_t)|y_t|^{-0.14}-1)/(-0.14)$.

## Assumption checking

```{r}
set.seed(0)

fit %>%
  select(auto) %>%
  gg_tsresiduals()

ARIMA.resid = augment(fit) %>%
  filter(.model == "auto") %>%
  select(.innov)

surrogate.test(ARIMA.resid, 8, 1000)$p.value
```

The surrogate test p-value of 0.998 suggests that the residuals are consistent
with white noise.

## Forecasting

```{r}
fc = fit %>% select(auto) %>% forecast(h=8)
hilo(fc, 95)
```



# 4. Neural network autoregression (NNAR) models
```{r, warning=FALSE}

fit <- train %>% 
  model(NNETAR(`Wholesale Trade`))

report(fit)

augment(fit)

train %>% 
  autoplot(`Wholesale Trade`, color = 'purple', alpha = 0.5) +
  geom_line(aes(y = augment(fit)$.fitted))

accuracy(fit)


```
Our neural network autoregressive model is NNAR(1,1,2)[4]:
- **NNAR(p,P,k)[m]**: This is the notation for the model, where:
  - `p`: Number of lagged inputs (autoregressive terms). Here, `p=1`.
  - `P`: Number of seasonal lags. Here, `P=1`.
  - `k`: Number of hidden nodes in the hidden layer. Here, `k=2`.
  - `m`: The periodicity of the time series data. Here, `m=4` indicate that it's quarterly data.


The model is an ensemble of 20 neural networks, and the predictions are averaged over these networks. Using multiple networks helps in reducing the variance of the predictions and improving robustness.

Each individual neural network in the ensemble has:
- **2 input nodes**: Corresponding to the lagged inputs and seasonal lags.
- **2 hidden nodes**: In the hidden layer.
- **1 output node**: Providing the forecasted value.

The model has a total of 9 weights, which include the weights connecting the input layer to the hidden layer and the hidden layer to the output layer, plus biases.

The output units of the neural network are linear, meaning the activation function used in the output layer is linear. This is typical for regression problems where the target variable is continuous.

The `sigma^2` (variance of the residuals) is estimated as 13233. This provides an indication of the model's error variance, which is a measure of how much the model's predictions deviate from the actual values.



# 5. Assumption checking
```{r}

```

# 6. Forecasting
```{r}

```

# 7. Member contributions






